{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeea47a-082e-4810-96c2-7bc3e244bb1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-Tuning the GPTJ-6B model using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691d819-deef-46e2-aeac-547f2a2c253a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will walk you through how to fine-tune a pre-trained large language model with domain specific knowledge. \n",
    "\n",
    "The domain specific dataset that we will be using to fine-tune this model will be from United Kingdom (U.K.) Supreme Court case documents. We will tune the model on roughly 693 legal documents. Because we are using transfer learning to train the model we are transfering the current model knowledge to a new model without having to train the entire GPTJ-6B model again. This allows us to shorten the time the model needs to train on the new model.\n",
    "\n",
    "### Prereqs\n",
    "\n",
    "To run this notebook we assume you have knowledge about running a SageMaker Notebook instance or SageMaker Studio Notebook instance.\n",
    "\n",
    "### SageMaker Studio Resources\n",
    "If you are unfamiliar with SageMaker Studio you can check out the resource links below.\n",
    "\n",
    "[Introduction to Amazon SageMaker Studio - Video](https://www.youtube.com/watch?v=YcJAc-x8XLQ)\n",
    "\n",
    "[Build ML models using SageMaker Studio Notebooks - Workshop - Video](https://www.youtube.com/watch?v=1iSiN4sVMjE)\n",
    "\n",
    "## Dataset info\n",
    "\n",
    "The stats. below are if you were to use all 693 case documents to tune the model.\n",
    "\n",
    "* <strong>Page count:</strong> ~17,718\n",
    "* <strong>Word count:</strong> 10,015,333\n",
    "* <strong>Characters (no spaces):</strong> 49,897,639\n",
    "\n",
    "The entire dataset is publically available and can be download [here](https://zenodo.org/record/7152317#.ZCSfaoTMI2y)\n",
    "\n",
    "## Using your own dataset\n",
    "You can refactor this notebook to use another dataset. You will need to update the dataset that is downloaded in the **Retrieve dataset** section to download your dataset. Lastly, you will need to refactor the code located in the **Creating Training dataset** section to create your training dataset based on your dataset format.\n",
    "\n",
    "## Considerations when fine-tuning the model\n",
    "The notebook has been configured to allow you to use only a subset of the entire dataset to fine-tune the model if desired. In the **Data Prep** section, there is a variable called **doc_count**. You can set this number to your preference, and the model will be fine-tuned based on that specific number of case from the dataset. The smaller the value you set for this variable, the faster the model will train/fine-tune.\n",
    "    \n",
    "## Training/Tuning Time estimates\n",
    "\n",
    "Here are the estimated training times based on total number of case documents in the training dataset. Note the training time is based on training for 3 epochs.\n",
    "\n",
    "#### All training was ran on 1 - *ml.p3dn.24xlarge* instance\n",
    "\n",
    "#### <strong>Training dataset document count </strong> 250\n",
    "Training time: 1 hour 41 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 500\n",
    "Training time: 2 hours 57 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 693\n",
    "Training time: 4 hours\n",
    "\n",
    "## GPTJ-6B base model\n",
    "\n",
    "Steps you will go through in the notebook to test the base model\n",
    "\n",
    "1. Clone this repo in a SageMaker Studio Jupyter notebook\n",
    "2. Install needed notebook libraries\n",
    "3. Configure the notebook to use SageMaker\n",
    "4. Retrieve base model container\n",
    "5. Deploy the model inference endpoint\n",
    "6. Call inference endpoint to retrieve results from the LLM\n",
    "\n",
    "## Fine-tuned model\n",
    "\n",
    "Steps you will go through in the notebook to test the fine-tuned model\n",
    "\n",
    "1. Download dataset\n",
    "2. Prep the dataset and upload it to S3\n",
    "3. Retrieve the base model container\n",
    "4. Set hyperparameters for fine-tuning\n",
    "5. Start training/tuning job\n",
    "6. Deploy inference endpoint for the fine-tuned model\n",
    "7. Call inference endpoint for the fine-tuned model\n",
    "8. Parse endpoint results\n",
    "\n",
    "### Final Step\n",
    "* Be sure you delete all models and endpoints to avoid incurring unneeded spend.\n",
    "    \n",
    "### Disclaimer\n",
    "This notebook demos how you can fine-tune an LLM using transfer learning. Even though this notebook is fine-tuned using actual (U.K.) Supreme Court case documents you should not use this notebook for legal advise.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba73fc7-29bc-4ef0-ae1b-84608c8916e4",
   "metadata": {},
   "source": [
    "## Install Pre Reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a147b-4f15-4aca-b114-5f7712b98a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d018169-b534-4a5c-9d3a-31478899048f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker SDK configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffff26-5802-4397-8302-aefe25762051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "account_id = sess.account_id()\n",
    "\n",
    "print(f\"SageMaker role that will be use: {aws_role}\")\n",
    "print(f\"AWS Account ID: {account_id}\")\n",
    "print(f\"AWS Region you are currently running in: {aws_region}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b553-65bd-4d12-946a-e4fa30092e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploying interence endpoint for the GPTJ-6 base model\n",
    "\n",
    "In this section we are deploying the HuggingFace GPTJ-6B base model in order to compare the inference results with the fine-tuned model we will tune later.\n",
    "\n",
    "The fine-tuned model will be trained on UK Supreme Court case documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2935e7a5-9772-467b-93a6-31a8cc62128a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Name of model being used\n",
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33fbd31d-c031-433e-94af-0b976e53cbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container location: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.3-cu117\n",
      "HuggingFace model location: s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.1.2/infer-prepack-huggingface-textgeneration1-gpt-j-6b.tar.gz\n",
      "----------------!Endpoint name: base-model-gptj-6B-huggingface-textgene-2023-08-08-00-38-59-767\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"base-model-gptj-6B-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "print(f\"Container location: {deploy_image_uri}\")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"HuggingFace model location: {model_uri}\")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the base model\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3861a00-903d-4fa9-b746-52646f60a33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "664b2203-ab30-4ed7-ac71-750d02ff4517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b9753-3541-4b17-b847-15bb93165f6d",
   "metadata": {},
   "source": [
    "## Call GPTJ-6B without fine-tuning the model\n",
    "In this section we make a call to the SageMaker inference point that host the base model that has not been fine-tuned and have the results returned back from the endpoint.\n",
    "\n",
    "After the results have been returned, it is recommended that you save them to a text file. This will allow you to compare the results against the fine-tuned model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "477372a9-0015-44d1-9102-9cae8350e930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Base model output\n",
      "His Honour Judge Richards: Thank you, Your Honour. I will direct you to the transcript in the file here, and I'll just put you on the basis of this.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "28\n",
      "SARATHA CHANNAKHANDANI, SR.I, PLAINTIFF: Your Honour, I'm reading this transcript. The man who is alleged to be my uncle.\n",
      "\n",
      "\n",
      "29.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "30.\n",
      "\n",
      "\n",
      "Suresh Dutt was never in court. He was never taken into custody. He was never found and the\n",
      "\n",
      "31.\n",
      "\n",
      "\n",
      "32.\n",
      "\n",
      "He was never convicted. He was never put on trial. There's no judgment of\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "It's not even an arrest warrant.\n",
      "\n",
      "\n",
      "\n",
      "33.\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "In this case, this\n",
      "\n",
      "\n",
      "transcript that's been given to us. The record of the trial was never given to us\n",
      "\n",
      "----------------------------\n",
      "End of base model output\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "\n",
    "# Note: We are looping through the array of queries and passing each query to the model inference endpoint to get model results\n",
    "# queries that will be passed to the model to answer\n",
    "for quota_text in [\n",
    "   \"His Honour Judge Richards\",\n",
    "]:\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    \n",
    "    print(\"Base model output\")\n",
    "    print(generated_texts)\n",
    "    print(\"\\n----------------------------\")\n",
    "    print(\"End of base model output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c71a7-ff21-4420-b825-5ad41dedeec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base model results\n",
    "The output above is what the base model will return to us before fine-tuning the model. As you can see the results are not great. The goal is to make the model give us better results after it has been fine-tuned on more data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4e794-8fdf-4d6c-88d2-24f78ef2451b",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Delete the SageMaker endpoint and the attached resources once you no longer endpoint them. The inteference endpoints incur cost if you leave them running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bda62-9280-4d43-8bfc-2c2b558ee750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e329fd-f742-4689-a7e6-64ec074f96f6",
   "metadata": {},
   "source": [
    "# Fine-Tuning the GPTJ-6 base model via transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7bc3d-3493-45a4-ac3f-0edc291a3a6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Retrieve dataset\n",
    "\n",
    "Download the dataset. This may take several minutes. The zipped dataset is 93 MB.\n",
    "\n",
    "To save time in the future you only need to run the download and unzip once. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34ecba-521a-41da-b8e3-75b71f193dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/record/7152317/files/dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397e3ca-d7fd-41e5-9c3d-d6db78f5f28f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unzipping compressed datasets\n",
    "# this may take several minutes since we are decompressing all the case files in the dataset\n",
    "\n",
    "print(\"Unzipping file. Wait for the dataset files to unzip. This may take several minutes ...\")\n",
    "\n",
    "!unzip -q dataset.zip\n",
    "\n",
    "print(\"Finished unzipping file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe2eaed7-6ac1-4e09-8394-ba0e9761fd00",
   "metadata": {},
   "source": [
    "## Configure file paths and storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "91437a2e-fbdb-4737-84e9-ab5fabe6d6cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'path/to/your/directory' with the actual path to your directory containing the text files\n",
    "training_path = 'dataset/UK-Abs/train-data/judgement'\n",
    "validation_path = 'dataset/UK-Abs/train-data/summary'\n",
    "\n",
    "local_training_file = 'dataset/train.txt'\n",
    "s3_training_file = \"train.txt\"\n",
    "s3_validation_file = \"validation.txt\"\n",
    "\n",
    "# Replace 'new_file.txt' with the name of the new file where you want to combine the contents\n",
    "training_file_path = 'dataset/train.txt'\n",
    "validation_file_path = 'dataset/validation.txt'\n",
    "\n",
    "model = \"gptj-6b\"\n",
    "bucket_name = f'sagemaker-{account_id}-{aws_region}' # change this to your bucket name and be sure it exist in S3\n",
    "training_folder = f'{model}/train' # the training folder in your bucket\n",
    "validation_folder = f'{model}/validation' # the training folder in your bucket\n",
    "\n",
    "s3_training_location = f\"s3://{bucket_name}/{training_folder}/\"\n",
    "s3_validation_location = f\"s3://{bucket_name}/{validation_folder}/\"\n",
    "s3_training_output_path = f\"s3://{bucket_name}/{model}/output\"\n",
    "\n",
    "training_dataset = f\"{s3_training_location}{s3_training_file}\"\n",
    "validation_dataset = f\"{s3_validation_location}{s3_validation_file}\"\n",
    "\n",
    "s3_output_location = f\"s3://{bucket_name}/{model}/output\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6323e4-2cb1-4525-876c-19810a173481",
   "metadata": {},
   "source": [
    "## Creating Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "abed6e70-bfe0-42aa-abaa-f4cb59ab9c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Local dataset dataset/train.txt has been created\n",
      "Local dataset dataset/validation.txt has been created\n"
     ]
    }
   ],
   "source": [
    "# doc_count is the number of documents to include the fine-tuning dataset\n",
    "# The higher the doc_count the larger the training dataset will be\n",
    "# The max document count is 693\n",
    "doc_count = 25\n",
    "\n",
    "def create_dataset(new_dataset_file, training_path, docs_in_dataset):\n",
    "    doc_in_dataset = 0\n",
    "    \n",
    "    with open(new_dataset_file, 'w') as new_file:\n",
    "        file_list = os.listdir(training_path)\n",
    "\n",
    "        for filename in file_list:\n",
    "            if doc_in_dataset < doc_count:\n",
    "                doc_in_dataset+=1\n",
    "                # Create the full file path by joining the directory path with the filename\n",
    "                file_path = os.path.join(training_path, filename)\n",
    "\n",
    "                # Check if the file is a regular file (not a directory)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Open the file in read mode\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        text_content = file.read()\n",
    "\n",
    "                    # Write the content of each file to the new file\n",
    "                    new_file.write(text_content)\n",
    "                    new_file.write(\"\\n-----------------------------------------------------------------\\n\")\n",
    "                    \n",
    "    print(f\"Local dataset {new_file.name} has been created\")\n",
    "\n",
    "# creats training dataset\n",
    "create_dataset(training_file_path, training_path, doc_count)\n",
    "\n",
    "# creats validation dataset\n",
    "create_dataset(validation_file_path, validation_path, doc_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3267fe0-19a4-4b30-84a7-67818e3e3e8a",
   "metadata": {},
   "source": [
    "## Upload training data to S3\n",
    "In this section we upload the dataset that was created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ba081-dc0a-46ac-96ad-9b9746913708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uploads training data to S3 so that model can be fine-tune using the dataset\n",
    "sagemaker_session.upload_data(local_training_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)\n",
    "\n",
    "sagemaker_session.upload_data(validation_file_path,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=validation_folder)\n",
    "\n",
    "print(f\"S3 Training location: {s3_training_location}\")\n",
    "print(f\"S3 Validation location: {s3_validation_location}\")\n",
    "print(f\"S3 Training dataset file: {training_dataset}\")\n",
    "print(f\"S3 Validation dataset file: {validation_dataset}\")\n",
    "print(f\"Model training output location: {s3_output_location}\")\n",
    "print(\"Training data uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66edbd-4d81-47e6-aa10-69dc18d7dd0b",
   "metadata": {},
   "source": [
    "## Setup Model to be tuned\n",
    "\n",
    "When selecting your instance type below ensure you have the minimal available to run based on your account quota. For some GPU based instances you may need to request an increase in the total number you can run in your account. This is true for spot instance type also which have a separate quota. \n",
    "\n",
    "You can request a service increase [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f44205-2436-4f4c-883d-9bb2eeacc67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\"\n",
    "\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "# you can change the instance to a smaller instance - https://aws.amazon.com/ec2/instance-types/p3/\n",
    "# training_instance_type = \"ml.p3dn.24xlarge\" \n",
    "\n",
    "training_instance_type = \"ml.g4dn.12xlarge\" \n",
    "\n",
    "# Retrieve the docker image for training\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_source_uri)\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958b68c-0e43-4488-8f0d-0f1c3c8957ef",
   "metadata": {},
   "source": [
    "## Spot Training configuration\n",
    "If **use_spot_instances** is set to **True** below training will use spot instances.\n",
    "\n",
    "Note: If you are using spot instances for training you will need to store training checkpoints in case your spot instances are shutdown. This allows you to continue training where you left off if spot instances are terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "51034e0d-7b4d-4802-b330-3cf89e49296c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint storage location: None\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "training_job_name = name_from_base(f\"{model_id}-transfer-learning\")\n",
    "\n",
    "# set use_spot_instances to true if you are going to use spot instances for training\n",
    "# ensure you have the proper quota for the instance type you set for the training_instance_type variable\n",
    "# you can check your quota here https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas \n",
    "# by enter the instance type you plan on using\n",
    "use_spot_instances = False\n",
    "max_run = 36000 # in seconds\n",
    "max_wait = 72000 if use_spot_instances else None # in seconds\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    # sets the location where training checkpoint will be stored if using spot instances\n",
    "    checkpoint_s3_uri = f'{s3_training_output_path}/checkpoints/{training_job_name}'\n",
    "    \n",
    "print (f'Checkpoint storage location: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f090726-2433-455b-abf2-7ef00b2e7b69",
   "metadata": {},
   "source": [
    "## Train with Automatic Model Tuning (HPO)\n",
    "This section configures Automatic Model Tuning if you change from **use_auto_tuning = False** to **use_auto_tuning = True**. By default we set it to false for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bcd4a283-f2e5-4bba-ab8e-321ae1111e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': '1', 'learning_rate': '6e-06', 'per_device_train_batch_size': '4', 'per_device_eval_batch_size': '8', 'warmup_ratio': '0.1', 'instruction_tuned': False, 'train_from_scratch': 'False', 'fp16': 'True', 'bf16': 'False', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '2', 'logging_steps': '10', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': 'False', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Set default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# Overriding default hyperparameters with custom values below\n",
    "\n",
    "# To make fine-tuning quicker we have set the epoch value to 1.\n",
    "hyperparameters[\"epoch\"] = \"1\"\n",
    "hyperparameters[\"per_device_train_batch_size\"] = \"4\"\n",
    "\n",
    "# If you are training with domain specific datasets you will need this parameter to be set to False\n",
    "hyperparameters[\"instruction_tuned\"] = False\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e75710-2c0a-4ed1-8c84-2d05040aa62c",
   "metadata": {},
   "source": [
    "## Set hyperparameters\n",
    "This section configures any hyperparameter if you decide to use automated model tuning. In this example we aren't using moel tuning, but the code exist if you would like to test out automated model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d2b12a8e-bc4b-4938-b4fc-d68dd51b3206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT (automated model tuning) for tuning and selecting the best model\n",
    "use_auto_tuning = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 2\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dedfd9-841e-4e8f-88a5-6cd17b548230",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "Here we start our SageMaker training job to tune the model. Depending on how much data is being used, the size of your training instance and the number of instances used for training will dictate how long it will take to train/tune your new model.\n",
    "\n",
    "If your training job fails because you surpassed your qouta for that instance type you can request an increase in your quota for that instance type [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas). You can request an instance quota increase for regular training instances and spot instances.\n",
    "\n",
    "You may also run into an error stating lack of capacity for your instance type. If you receive this type of error you can re-run the cell until the training job starts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5a1fa-0461-4191-8121-b07c829d2dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "# defines model metrics that are used to evaluate the models performance\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait\n",
    ")\n",
    "\n",
    "# checks to see if you are using automated model tuning\n",
    "if use_auto_tuning:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name\n",
    "    )\n",
    "    \n",
    "    print(\"Using hyerparameter tuning job\")\n",
    "    # Start a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": s3_training_location, \"validation\": s3_validation_location }, logs=True)\n",
    "else:\n",
    "    print(f\"Training file location is {s3_training_location}\")\n",
    "    print(f\"Validation file location is {s3_validation_location}\")\n",
    "    \n",
    "    # Start a SageMaker Training job by passing s3 path for the training dataset\n",
    "    tg_estimator.fit({\"train\": s3_training_location, \"validation\": s3_validation_location}, logs=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd7886-1895-4b93-932b-a41d9c97fdc8",
   "metadata": {},
   "source": [
    "## Review Training metrics\n",
    "Here we output the training metrics returned from the training job."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be261a3-68ef-4f9f-9683-363ad2e3677f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_auto_tuning:\n",
    "    print(\"Getting the best trained model from the hyperparameter tuner\")\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ac4f7-0250-406e-a55a-13ba4c71bb8c",
   "metadata": {},
   "source": [
    "## Deploy & run Inference on the fine-tuned model\n",
    "In this section we are deploying a model inference endpoint so that we can run inferences against the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a9d7453f-681c-45c1-b171-f5739e775662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g4dn.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-08-08-01-27-29-320\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: fine-tuned-huggingface-textgeneration1--2023-08-08-01-27-29-319\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name fine-tuned-huggingface-textgeneration1--2023-08-08-01-27-29-319\n",
      "INFO:sagemaker:Creating endpoint with name fine-tuned-huggingface-textgeneration1--2023-08-08-01-27-29-319\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.g4dn.12xlarge\"\n",
    "\n",
    "# Retrieve the docker container uri for inference\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"fine-tuned-{model_id}\")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name_after_finetune}\" )\n",
    "\n",
    "# Deploy to SageMaker inference endpoint\n",
    "finetuned_predictor = (hp_tuner if use_auto_tuning else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc72ef4-217d-4f43-a7e0-61db33794f0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "2fa6c064-11c3-41aa-a5d7-8522ace181dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe077daf-092f-4349-bf56-3011a24fbce4",
   "metadata": {},
   "source": [
    "## Calling fine-tuned model\n",
    "Once the results are returned, you should notice that the fine-tuned model returns better results. In this case, we are using the same queries that were passed to the model that was not fine-tuned. This allows you to easy comparison the results between the two models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b5fba258-0977-4a4f-804f-146124eb8c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuned model output\n",
      "His Honour Judge Richards: This is a case which concerns the issue of whether or not it is in the best interests of the child to maintain the parent/child relationship.I will address that question in this judgment.The parties were married on 11 August 2001.There were no children of the marriage, but Ms Parmer and her new partner, Mr Green, have three children.In 2007 Ms Parmer had her first child with Mr Green, and, shortly after the birth of the child, she became pregnant again.This time, she and Mr Green had a daughter.Ms Parmer has an older child with Mr Green's first child, and a younger child with Mr Green's second child.Mr Green lives with his mother.Ms Parmer is divorced from her first husband.On 10 July 2010, Ms Parmer commenced this application for an order under the Family Law Act 1975, section 15, seeking to have Mr Green declared to be the father of her younger child.In his written answer\n",
      "\n",
      "------------------------------------------------------------------\n",
      "End of fine-tuned model output\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 200,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "    \n",
    "# Note: We are looping through the array of queries and passing each query to the model inference endpoint to get model results\n",
    "# queries that will be passed to the fine-tuned model.\n",
    "for quota_text in [\n",
    "    \"His Honour Judge Richards\",\n",
    "]:\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "    \n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name_after_finetune\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    \n",
    "    print(\"Fine-tuned model output\")\n",
    "    print(generated_texts)\n",
    "    print(\"\\n------------------------------------------------------------------\")\n",
    "    print(\"End of fine-tuned model output\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fed936-1acf-4d47-acc8-43ca10b0c8c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean-Up\n",
    "Here we are performing clean-up by deleting the fine-tuned model and deleting the inference endpoint that was deployed.\n",
    "\n",
    "Note: Leaving an inference endpoint running can be costly depending on the instance type you deployed your endpoint to. In this notebook we are using the ml.g5.12xlarge instance type for our inference endpoint. This is a GPU based instance which cost roughly $5.672 per hour to run at this time of publishing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec6142-d74f-4a40-8463-244e86c8c87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c1312a8-13c9-405e-8197-93ff6c398b48",
   "metadata": {},
   "source": [
    "### Disclaimer\n",
    "This notebook demos how you can fine-tune an LLM using transfer learning. Even though this notebook is fine-tuned using actual (U.K.) Supreme Court case documents you should not use this notebook for legal advise."
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
