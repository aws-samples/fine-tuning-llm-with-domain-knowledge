{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeea47a-082e-4810-96c2-7bc3e244bb1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Fine-Tuning the GPTJ-6B model using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691d819-deef-46e2-aeac-547f2a2c253a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will walk you through how to fine-tune a pre-trained large language model with domain specific knowledge. \n",
    "\n",
    "The domain specific dataset that we will be using to fine-tune this model will be United Kingdom (U.K.) Supreme Court case documents. There is roughly 693 legal case documents in the training dataset.\n",
    "\n",
    "## Dataset info\n",
    "* <strong>Page count:</strong> ~17,718\n",
    "* <strong>Word count:</strong> 10,015,333\n",
    "* <strong>Characters (no spaces):</strong> 49,897,639\n",
    "\n",
    "The entire dataset is publically available and can be download [here](https://zenodo.org/record/7152317#.ZCSfaoTMI2y)\n",
    "\n",
    "## Considerations when fine-tuning the model\n",
    "The notebook has been configured to allow you to only use a subset of the entire dataset to fine-tune the model if you would like. There is a variable named _**doc_count**_ in the _**Data Prep**_ section. You can set this number to whatever you would like and it will only fine-tune the model based on the number of documents you set this variable to. The smaller this value the faster the model will fine-tune.\n",
    "    \n",
    "## Training/Tuning Time estimates\n",
    "\n",
    "Here are the estimated training times based on total number of case documents in the training dataset.\n",
    "\n",
    "#### All training was ran on 1 - *ml.p3dn.24xlarge* instance\n",
    "\n",
    "#### <strong>Training dataset document count </strong> 250\n",
    "Training time: 1 hour 41 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 500\n",
    "Training time: 2 hours 57 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 693\n",
    "Training time: 4 hours\n",
    "\n",
    "## GPTJ-6B base model\n",
    "\n",
    "Steps you will go through to test the base model\n",
    "\n",
    "1. Install needed notebook libraries\n",
    "3. Configure the notebook to use SageMaker\n",
    "4. Retrieve base model container\n",
    "5. Deploy the model inference endpoint\n",
    "6. Call inference endpoint to retrieve results from the LLM\n",
    "\n",
    "## Fine-tuned model\n",
    "\n",
    "Steps you will go through to test the fine-tuned model\n",
    "\n",
    "1. Download dataset\n",
    "2. Prep the dataset and upload it to S3\n",
    "3. Retrieve the base model container\n",
    "4. Set hyperparameters for fine-tuning\n",
    "5. Start training/tuning job\n",
    "6. Deploy inference endpoint for the fine-tuned model\n",
    "7. Call inference endpoint for the fine-tuned model\n",
    "8. Parse endpoint results\n",
    "\n",
    "### Final Step\n",
    "* Be sure you delete all models and endpoints to avoid incurring unneeded spend.\n",
    "    \n",
    "### Disclaimer\n",
    "This notebook demos how you can fine-tune an LLM using transfer learning. Even though this notebook is fine-tuned using actual (U.K.) Supreme Court case documents you should not use this notebook for legal advise.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba73fc7-29bc-4ef0-ae1b-84608c8916e4",
   "metadata": {},
   "source": [
    "## Install Pre Reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a147b-4f15-4aca-b114-5f7712b98a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d018169-b534-4a5c-9d3a-31478899048f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker SDK configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffff26-5802-4397-8302-aefe25762051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "account_id = sess.account_id()\n",
    "\n",
    "print(f\"Role SageMaker will use - {aws_role}\")\n",
    "print(f\"AWS Account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b553-65bd-4d12-946a-e4fa30092e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploying interence endpoint for the GPTJ-6 base model\n",
    "\n",
    "In this section we are deploying the HuggingFace GPTJ-6B base model in order to compare the inference results with the fine-tuned model we will tune later.\n",
    "\n",
    "The fine-tuned model will be trained on UK Supreme Court case documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2935e7a5-9772-467b-93a6-31a8cc62128a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Name of model being used\n",
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33fbd31d-c031-433e-94af-0b976e53cbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"base-model-gptj-6B-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "print(f\"Container location: {deploy_image_uri}\")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"Model location: {model_uri}\")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the base model\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3861a00-903d-4fa9-b746-52646f60a33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "664b2203-ab30-4ed7-ac71-750d02ff4517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b9753-3541-4b17-b847-15bb93165f6d",
   "metadata": {},
   "source": [
    "## Call GPTJ-6B inference endpoint\n",
    "In this section we make a call to the SageMaker inference point that host the base model and have the results returned back from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "477372a9-0015-44d1-9102-9cae8350e930",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "\n",
    "for quota_text in [\n",
    "    # \"Tell me about the Matrimonial and Family Proceedings Act 1984\",\n",
    "    \"tell me about the Ellenborough Park [1956] case\",\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c71a7-ff21-4420-b825-5ad41dedeec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base model results\n",
    "The output above is what the base model will return to use before fine-tuning the model. It will only return with data that it knows about when the model was pre-trained. The goal is to make the model give us better results after it has more context based on case law that it will be fine-tuned with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4e794-8fdf-4d6c-88d2-24f78ef2451b",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Delete the SageMaker endpoint and the attached resources once you no longer endpoint them. The inteference endpoints incur cost if you leave them running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe0bda62-9280-4d43-8bfc-2c2b558ee750",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e329fd-f742-4689-a7e6-64ec074f96f6",
   "metadata": {},
   "source": [
    "# Fine-Tuning the GPTJ-6 base model via transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7bc3d-3493-45a4-ac3f-0edc291a3a6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "Download the dataset. This may take several minutes. The zipped dataset is 93 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34ecba-521a-41da-b8e3-75b71f193dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/record/7152317/files/dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397e3ca-d7fd-41e5-9c3d-d6db78f5f28f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unzipping compressed datasets\n",
    "# this may take several minutes since we are decompressing all the case files in the dataset\n",
    "\n",
    "print(\"unzipping file...\")\n",
    "\n",
    "!unzip -q dataset.zip\n",
    "\n",
    "print(\"finished unzipping file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6323e4-2cb1-4525-876c-19810a173481",
   "metadata": {},
   "source": [
    "## Creating Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abed6e70-bfe0-42aa-abaa-f4cb59ab9c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'path/to/your/directory' with the actual path to your directory containing the text files\n",
    "directory_path = 'dataset/UK-Abs/train-data/judgement'\n",
    "train_file = 'dataset/train.txt'\n",
    "s3_file = \"train.txt\"\n",
    "\n",
    "# Replace 'new_file.txt' with the name of the new file where you want to combine the contents\n",
    "new_file_path = 'dataset/train.txt'\n",
    "\n",
    "bucket_name = f'sagemaker-{account_id}-{aws_region}' # change this to your bucket name and be sure it exist in S3\n",
    "training_folder = r'training_dataset' # the training folder in your bucket\n",
    "\n",
    "# number of documents to include the fine-tuning dataset\n",
    "# The higher the doc_count the larger the training dataset will be\n",
    "# The max document count is 693\n",
    "doc_count = 100\n",
    "\n",
    "doc_in_dataset = 0\n",
    "\n",
    "# Loop through each file and append its content to the new file\n",
    "with open(new_file_path, 'w') as new_file:\n",
    "    file_list = os.listdir(directory_path)\n",
    "    for filename in file_list:\n",
    "        if doc_in_dataset < doc_count:\n",
    "            doc_in_dataset+=1\n",
    "            # Create the full file path by joining the directory path with the filename\n",
    "            file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "            # Check if the file is a regular file (not a directory)\n",
    "            if os.path.isfile(file_path):\n",
    "                # Open the file in read mode\n",
    "                with open(file_path, 'r') as file:\n",
    "                    text_content = file.read()\n",
    "\n",
    "                # Write the content of each file to the new file\n",
    "                new_file.write(text_content)\n",
    "                new_file.write(\"\\n-----------------------------------------------------------------\\n\")\n",
    "            \n",
    "print(\"Training dataset created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3267fe0-19a4-4b30-84a7-67818e3e3e8a",
   "metadata": {},
   "source": [
    "## Upload training data to S3\n",
    "In this section we upload the dataset that was created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ba081-dc0a-46ac-96ad-9b9746913708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uploads training data to S3 so that model can be fine-tune using the dataset\n",
    "sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)\n",
    "\n",
    "print(f\"Training Dataset: s3://{bucket_name}/{training_folder}/{s3_file}\")\n",
    "print(\"Training data uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66edbd-4d81-47e6-aa10-69dc18d7dd0b",
   "metadata": {},
   "source": [
    "## Setup Model to be tuned\n",
    "\n",
    "When selecting your instance type below ensure you have the minimal available to run based on your account quota. For some GPU based instances you may need to request an increase in the total number you can run in your account. This is true for spot instance type also which have a separate quota. \n",
    "\n",
    "You can request a service increase [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0f44205-2436-4f4c-883d-9bb2eeacc67f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\"\n",
    "\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "# you can change the instance to a smaller instance - https://aws.amazon.com/ec2/instance-types/p3/\n",
    "training_instance_type = \"ml.p3dn.24xlarge\" \n",
    "\n",
    "# G4 instance https://aws.amazon.com/ec2/instance-types/g4/\n",
    "# training_instance_type = \"ml.g4dn.12xlarge\" \n",
    "\n",
    "# training_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the docker image for training\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_source_uri)\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73892e-1f0f-460f-9341-ab9670013d69",
   "metadata": {},
   "source": [
    "## Configure storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b509def4-9ad9-46f6-a296-1584783b0b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# bucket name \n",
    "bucket_name = f'sagemaker-{account_id}-{aws_region}'\n",
    "\n",
    "# location where training dataset will reside\n",
    "training_dataset_s3_path = f\"s3://{bucket_name}/training_dataset/\" \n",
    "\n",
    "# location where validation data will exist\n",
    "validation_dataset_s3_path = f\"s3://{bucket_name}/training/validation/\"\n",
    "\n",
    "output_prefix = \"training_output\"\n",
    "\n",
    "s3_output_location = f\"s3://{bucket_name}/{output_prefix}/output\"\n",
    "\n",
    "print(f\"Training dataset location: {training_dataset_s3_path}\")\n",
    "print(f\"Validation dataset location: {validation_dataset_s3_path}\")\n",
    "print(f\"Model training output location: {s3_output_location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958b68c-0e43-4488-8f0d-0f1c3c8957ef",
   "metadata": {},
   "source": [
    "## Spot Training configuration\n",
    "If **use_spot_instances** is set to **True** below training will use spot instances.\n",
    "\n",
    "Note: If you are using spot instances for training you will need to store training checkpoints in case your spot instances are shutdown. This allows you to continue training where you left off if spot instances are terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51034e0d-7b4d-4802-b330-3cf89e49296c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "training_job_name = name_from_base(f\"{model_id}-transfer-learning\")\n",
    "\n",
    "# set to true if you are going to use spot instances for training\n",
    "use_spot_instances = False\n",
    "max_run = 36000 # in seconds\n",
    "max_wait = 7200 if use_spot_instances else None # in seconds\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    # sets the location where training checkpoint will be stored if using spot instances\n",
    "    checkpoint_s3_uri = f's3://{bucket_name}/{output_prefix}/checkpoints/{training_job_name}'\n",
    "    \n",
    "print (f'Checkpoint storage location: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f090726-2433-455b-abf2-7ef00b2e7b69",
   "metadata": {},
   "source": [
    "## Train with Automatic Model Tuning (HPO)\n",
    "This section configures Automatic Model Tuning if you change from **use_amt = False** to **use_amt = True**. By default we set it to false for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcd4a283-f2e5-4bba-ab8e-321ae1111e60",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Set default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# [Optional] Override default hyperparameters with custom values\n",
    "hyperparameters[\"epoch\"] = \"3\"\n",
    "hyperparameters[\"per_device_train_batch_size\"] = \"4\"\n",
    "\n",
    "# If you training with domain specific datasets you will be to set this to False\n",
    "hyperparameters[\"instruction_tuned\"] = False\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e75710-2c0a-4ed1-8c84-2d05040aa62c",
   "metadata": {},
   "source": [
    "## Set hyperparameters\n",
    "This section configures any hyperparameter if you decide to use automated model tuning. In this example we are not but the code exist if you would like to test out automated model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2b12a8e-bc4b-4938-b4fc-d68dd51b3206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT (automated model tuning) for tuning and selecting the best model\n",
    "use_amt = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 6\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dedfd9-841e-4e8f-88a5-6cd17b548230",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "Here we start our SageMaker training job to tune the model. Depending on how much data is being used, the size of your training instance and the number of instances used for training will dictate how long it will take to train/tune your new model.\n",
    "\n",
    "If your training job fails because you surpassed your qouta for that instance type you can request an increase in your quota for that instance type [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas). You can request an instance quota increase for regular training instances and spot instances.\n",
    "\n",
    "You may also run into an error stating lack of capacity for your instance type. If you receive this type of error you can re-run the cell until the training job starts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5a1fa-0461-4191-8121-b07c829d2dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "# defines model metrics that are used to evaluate the models performance\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait\n",
    ")\n",
    "\n",
    "print(f\"Training dataset location: {training_dataset_s3_path}\")\n",
    "\n",
    "# chceks to see if you are using automated model tuning\n",
    "if use_amt:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name,\n",
    "        instruction_tuned=False\n",
    "    )\n",
    "\n",
    "    # Start a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path })\n",
    "else:\n",
    "    # Start a SageMaker Training job by passing s3 path for the training dataset\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd7886-1895-4b93-932b-a41d9c97fdc8",
   "metadata": {},
   "source": [
    "## Review Training metrics\n",
    "Here we output the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1be261a3-68ef-4f9f-9683-363ad2e3677f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_amt:\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ac4f7-0250-406e-a55a-13ba4c71bb8c",
   "metadata": {},
   "source": [
    "## Deploy & run Inference on the fine-tuned model\n",
    "In this section we are deploying a model inference endpoint so that we can run inferences against the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9d7453f-681c-45c1-b171-f5739e775662",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "inference_instance_type = \"ml.g4dn.12xlarge\"\n",
    "\n",
    "# Retrieve the docker container uri for inference\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"fine-tuned-{model_id}\")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name_after_finetune}\" )\n",
    "\n",
    "# Deploy to SageMaker inference endpoint\n",
    "finetuned_predictor = (hp_tuner if use_amt else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc72ef4-217d-4f43-a7e0-61db33794f0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fa6c064-11c3-41aa-a5d7-8522ace181dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5fba258-0977-4a4f-804f-146124eb8c95",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "    \n",
    "for quota_text in [\n",
    "    # \"Tell me about the Matrimonial and Family Proceedings Act 1984\",\n",
    "    \"tell me about the Ellenborough Park [1956] case\"\n",
    "]:\n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "    \n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name_after_finetune\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(\"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fed936-1acf-4d47-acc8-43ca10b0c8c9",
   "metadata": {},
   "source": [
    "## Clean-Up\n",
    "Here we are performing clean-up by deleting the fine-tuned model and deleting the inference endpoint that was deployed.\n",
    "\n",
    "Note: Leaving an inference endpoint running can be costly depending on the instance type you deployed your endpoint to. In this notebook we are using the ml.g5.12xlarge instance type for our inference endpoint. This is a GPU based instance which cost roughly $5.672 per hour to run at this time of publishing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44ec6142-d74f-4a40-8463-244e86c8c87c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
