{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "cbeea47a-082e-4810-96c2-7bc3e244bb1c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Fine-Tuning the GPTJ-6B model using transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9691d819-deef-46e2-aeac-547f2a2c253a",
   "metadata": {},
   "source": [
    "## Overview\n",
    "This notebook will walk you through how to fine-tune a pre-trained large language model with domain specific knowledge. \n",
    "\n",
    "The domain specific dataset that we will be using to fine-tune this model will be United Kingdom (U.K.) Supreme Court case documents. We will tune the model on roughly 693 legal documents. \n",
    "\n",
    "## Dataset info\n",
    "* <strong>Page count:</strong> ~17,718\n",
    "* <strong>Word count:</strong> 10,015,333\n",
    "* <strong>Characters (no spaces):</strong> 49,897,639\n",
    "\n",
    "The entire dataset is publically available and can be download [here](https://zenodo.org/record/7152317#.ZCSfaoTMI2y)\n",
    "\n",
    "## Considerations when fine-tuning the model\n",
    "The notebook has been configured to allow you to only use a subset of the entire dataset to fine-tune the model if you would like. There is a variable named _**doc_count**_ in the _**Data Prep**_ section. You can set this number to whatever you would like and it will only fine-tune the model based on the number of documents you set this variable to. The smaller this value the faster the model will fine-tune.\n",
    "    \n",
    "## Training/Tuning Time estimates\n",
    "\n",
    "Here are the estimated training times based on total number of case documents in the training dataset.\n",
    "\n",
    "#### All training was ran on 1 - *ml.p3dn.24xlarge* instance\n",
    "\n",
    "#### <strong>Training dataset document count </strong> 250\n",
    "Training time: 1 hour 41 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 500\n",
    "Training time: 2 hours 57 minutes\n",
    "\n",
    "#### <strong>Training document count</strong> 693\n",
    "Training time: 4 hours\n",
    "\n",
    "## GPTJ-6B base model\n",
    "\n",
    "Steps you will go through in the notebook to test the base model\n",
    "\n",
    "1. Clone this repo in a SageMaker Studio Jupyter notebook\n",
    "2. Install needed notebook libraries\n",
    "3. Configure the notebook to use SageMaker\n",
    "4. Retrieve base model container\n",
    "5. Deploy the model inference endpoint\n",
    "6. Call inference endpoint to retrieve results from the LLM\n",
    "\n",
    "## Fine-tuned model\n",
    "\n",
    "Steps you will go through in the notebook to test the fine-tuned model\n",
    "\n",
    "1. Download dataset\n",
    "2. Prep the dataset and upload it to S3\n",
    "3. Retrieve the base model container\n",
    "4. Set hyperparameters for fine-tuning\n",
    "5. Start training/tuning job\n",
    "6. Deploy inference endpoint for the fine-tuned model\n",
    "7. Call inference endpoint for the fine-tuned model\n",
    "8. Parse endpoint results\n",
    "\n",
    "### Final Step\n",
    "* Be sure you delete all models and endpoints to avoid incurring unneeded spend.\n",
    "    \n",
    "### Disclaimer\n",
    "This notebook demos how you can fine-tune an LLM using transfer learning. Even though this notebook is fine-tuned using actual (U.K.) Supreme Court case documents you should not use this notebook for legal advise.\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fba73fc7-29bc-4ef0-ae1b-84608c8916e4",
   "metadata": {},
   "source": [
    "## Install Pre Reqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f5a147b-4f15-4aca-b114-5f7712b98a4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install --upgrade sagemaker --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d018169-b534-4a5c-9d3a-31478899048f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## SageMaker SDK configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1ffff26-5802-4397-8302-aefe25762051",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import sagemaker, boto3, json\n",
    "from sagemaker.session import Session\n",
    "\n",
    "sagemaker_session = Session()\n",
    "aws_role = sagemaker_session.get_caller_identity_arn()\n",
    "aws_region = boto3.Session().region_name\n",
    "sess = sagemaker.Session()\n",
    "account_id = sess.account_id()\n",
    "\n",
    "print(f\"Role SageMaker will use - {aws_role}\")\n",
    "print(f\"AWS Account ID: {account_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04f0b553-65bd-4d12-946a-e4fa30092e87",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Deploying interence endpoint for the GPTJ-6 base model\n",
    "\n",
    "In this section we are deploying the HuggingFace GPTJ-6B base model in order to compare the inference results with the fine-tuned model we will tune later.\n",
    "\n",
    "The fine-tuned model will be trained on UK Supreme Court case documents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2935e7a5-9772-467b-93a6-31a8cc62128a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Name of model being used\n",
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "33fbd31d-c031-433e-94af-0b976e53cbf6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g5.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Container location: 763104351884.dkr.ecr.us-east-1.amazonaws.com/djl-inference:0.21.0-deepspeed0.8.3-cu117\n",
      "Model location: s3://jumpstart-cache-prod-us-east-1/huggingface-infer/prepack/v1.1.2/infer-prepack-huggingface-textgeneration1-gpt-j-6b.tar.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n",
      "INFO:sagemaker:Creating endpoint with name base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------!Endpoint name: base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import image_uris, model_uris, script_uris\n",
    "from sagemaker.model import Model\n",
    "from sagemaker.predictor import Predictor\n",
    "from sagemaker.utils import name_from_base\n",
    "\n",
    "endpoint_name = name_from_base(f\"base-model-gptj-6B-{model_id}\")\n",
    "\n",
    "inference_instance_type = \"ml.g5.12xlarge\"\n",
    "\n",
    "# Retrieve the inference docker container uri.\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "print(f\"Container location: {deploy_image_uri}\")\n",
    "\n",
    "# Retrieve the model uri.\n",
    "model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"inference\"\n",
    ")\n",
    "\n",
    "print(f\"Model location: {model_uri}\")\n",
    "\n",
    "# Create the SageMaker model instance. Note that we need to pass Predictor class when we deploy model through Model class,\n",
    "# for being able to run inference through the sagemaker API.\n",
    "model = Model(\n",
    "    image_uri=deploy_image_uri,\n",
    "    model_data=model_uri,\n",
    "    role=aws_role,\n",
    "    predictor_cls=Predictor,\n",
    "    name=endpoint_name,\n",
    ")\n",
    "\n",
    "# deploy the base model\n",
    "base_model_predictor = model.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    endpoint_name=endpoint_name,\n",
    ")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name}\" )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3861a00-903d-4fa9-b746-52646f60a33b",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "664b2203-ab30-4ed7-ac71-750d02ff4517",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49b9753-3541-4b17-b847-15bb93165f6d",
   "metadata": {},
   "source": [
    "## Call GPTJ-6B inference endpoint\n",
    "In this section we make a call to the SageMaker inference point that host the base model and have the results returned back from the endpoint."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "477372a9-0015-44d1-9102-9cae8350e930",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the Matrimonial and Family Proceedings Act 1984: The Act seeks to eliminate the differences between the civil and criminal courts. It provides for a common set of rules to apply to all matters which concern family relationships.\n",
      "\n",
      "\n",
      "\n",
      "3. Does it cover all family matters in England and Wales?\n",
      "\n",
      "\n",
      "\n",
      "4. Are the courts able to apply family law and criminal law in the same way?\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "5. Is it a Family Act?\n",
      "6. Can it be used by the courts to make orders for a person, not just in the court case but also in matters relating to civil law?\n",
      "\n",
      "1. The Act was created as a result of the problems and failures in the previous Acts of Parliament that were brought in by the Judicial Review that the Court of Appeal in 1984.\n",
      "\n",
      "Acts\n",
      "\n",
      "2. So, what are these, and what are the following Actâ€™s are:\n",
      "\n",
      "Civil Law:\n",
      "3. Family law was a separate law.\n",
      "4. This Act only concern the proceedings.\n",
      "5. Is not covered in the Criminal Justice Act 1988.\n",
      "\n",
      "Section 15 of the Court of Appeal, the High Court\n",
      "In the Family Proceedings: and\n",
      "6. No, Family Proceedings.\n",
      "\n",
      "7. Not in the Human Rights Act\n",
      "1984.\n",
      "\n",
      "8. It covers Family Law and Child Law.\n",
      "9. Why was the Act created?\n",
      "10. The Act of Parliament the of the Family Law, which is now the Family Proceedings, and Law.\n",
      "\n",
      "11. Are they covered by the Act and other statutes, the Court.\n",
      "12. If a case in that can apply the law?\n",
      "\n",
      "\n",
      "13. If it was originally made for the courts.\n",
      "\n",
      "14. No, it is the court\n",
      "The Act can also be found at:\n",
      "15. This Act not to criminal proceedings\n",
      "16. Not sure about this,\n",
      "\n",
      "17. There is an appeal against orders for the court.\n",
      "18. No, no, not a court of appeal, in the\n",
      "19.\n",
      "\n",
      "20. The Act has taken effect,\n",
      "21. Which was put in\n",
      "\n",
      "22. No it.\n",
      "23. Is a remedy.\n",
      "24. Is also a statute:\n",
      "25. No, and not to criminal law.\n",
      "26. Is a Family Proceedings Act, 1985\n",
      "\n",
      "27. No, the Act, the\n",
      "28.\n",
      "\n",
      "----------------------------\n",
      "tell me about the Ellenborough Park [1956] case: how did you think it would turn out?\n",
      "\n",
      "a) an appeal in the High Court\n",
      "b) a conviction, but an appeal\n",
      "c) an acquittal, on a technicality, or a technicality\n",
      "d) the jury is divided in their verdict, and some cannot agree\n",
      "\n",
      "a) 'I did not think the verdict would be 'guilty of manslaughter', or 'guilty of murder'\n",
      "b) '... because you are going to the gallows', 'because you have killed him [the man]'\n",
      "c) '... as you have already killed him [man], you shall not be found guilty of murder, but of manslaughter'\n",
      "\n",
      "d) '...in order to save the King's life'.\n",
      "\n",
      "1.4.16\n",
      "\n",
      "A: What are the three main causes of a legal argument?\n",
      "\n",
      "a) technicalities\n",
      "b) general verdicts\n",
      "c) particular verdicts\n",
      "\n",
      "[1956] I think I will go to the Law Courts\n",
      "a) to find a verdict\n",
      "b) to defend the king's person from [your country]\n",
      "c) to do something for the king's person, and the king's person\n",
      "\n",
      "d) because the jury have given a verdict, and the people in the same jury will\n",
      "\n",
      "1.4.18\n",
      "\n",
      "[1956] A: [The King] is an English monarch. [1956] A: What are the two main causes of the cause of [1956] a legal argument?\n",
      "1.2.18\n",
      "a) 'the jury have found him [defendant] not guilty'.\n",
      "b) 'to save the king's life'\n",
      "c) 'to keep the king safe'\n",
      "\n",
      "D: That's all you want to know about law. The king, is not to be found guilty of murder\n",
      "\n",
      "\n",
      "1.4.19.18\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "a) [1956]\n",
      "\n",
      "\n",
      "A: To know that you should not say that you have not told the court:\n",
      "A: I do not know\n",
      "b) 'it's a) all the men, not found that [1956] B.C: What is a technicality?\n",
      "B: He's not going to the gallows [1956] A: I do not think it should be an appeal\n",
      "A: The\n",
      "\n",
      "----------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_before_finetune = []\n",
    "\n",
    "for quota_text in [\n",
    "    \"Tell me about the Matrimonial and Family Proceedings Act 1984\",\n",
    "    \"tell me about the Ellenborough Park [1956] case\",\n",
    "    # \"what was the case about that involved The Palmers Wood Oil Field\",\n",
    "    # \"what was the case about that involved Mohammed Jabar Ahmed, Mohammed Azmir Khan and Michael Marteen\",\n",
    "    # \"Tel me about Thirteenth Protocol to the European Convention on Human Rights (2004)\",\n",
    "]:\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "\n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name=endpoint_name\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_before_finetune.append(generated_texts)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(\"\\n----------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b11c71a7-ff21-4420-b825-5ad41dedeec5",
   "metadata": {
    "tags": []
   },
   "source": [
    "### Base model results\n",
    "The output above is what the base model will return to use before fine-tuning the model. It will only return with data that it knows about when the model was pre-trained. The goal is to make the model give us better results after it has more context based on case law that it will be fine-tuned with."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56b4e794-8fdf-4d6c-88d2-24f78ef2451b",
   "metadata": {},
   "source": [
    "## Clean-up\n",
    "\n",
    "Delete the SageMaker endpoint and the attached resources once you no longer endpoint them. The inteference endpoints incur cost if you leave them running."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fe0bda62-9280-4d43-8bfc-2c2b558ee750",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n",
      "INFO:sagemaker:Deleting endpoint with name: base-model-gptj-6B-huggingface-textgene-2023-08-06-20-33-12-552\n"
     ]
    }
   ],
   "source": [
    "base_model_predictor.delete_model()\n",
    "base_model_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93e329fd-f742-4689-a7e6-64ec074f96f6",
   "metadata": {},
   "source": [
    "# Fine-Tuning the GPTJ-6 base model via transfer learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2d7bc3d-3493-45a4-ac3f-0edc291a3a6c",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Data Prep\n",
    "\n",
    "Download the dataset. This may take several minutes. The zipped dataset is 93 MB."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf34ecba-521a-41da-b8e3-75b71f193dc5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!wget https://zenodo.org/record/7152317/files/dataset.zip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8397e3ca-d7fd-41e5-9c3d-d6db78f5f28f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# unzipping compressed datasets\n",
    "# this may take several minutes since we are decompressing all the case files in the dataset\n",
    "\n",
    "print(\"Unzipping file. Wait for the dataset files to unzip. This may take several minutes ...\")\n",
    "\n",
    "!unzip -q dataset.zip\n",
    "\n",
    "print(\"Finished unzipping file\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f6323e4-2cb1-4525-876c-19810a173481",
   "metadata": {},
   "source": [
    "## Creating Training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "abed6e70-bfe0-42aa-abaa-f4cb59ab9c4e",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Datasets created\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Replace 'path/to/your/directory' with the actual path to your directory containing the text files\n",
    "directory_path = 'dataset/UK-Abs/train-data/judgement'\n",
    "validation_path = 'dataset/UK-Abs/train-data/summary'\n",
    "\n",
    "train_file = 'dataset/train.txt'\n",
    "\n",
    "s3_file = \"train.txt\"\n",
    "s3_validation_file = \"validation.txt\"\n",
    "\n",
    "# Replace 'new_file.txt' with the name of the new file where you want to combine the contents\n",
    "new_file_path = 'dataset/train.txt'\n",
    "validation_file_path = 'dataset/validation.txt'\n",
    "\n",
    "bucket_name = f'sagemaker-{account_id}-{aws_region}' # change this to your bucket name and be sure it exist in S3\n",
    "training_folder = r'training_dataset' # the training folder in your bucket\n",
    "validation_folder = r'validation_dataset' # the training folder in your bucket\n",
    "\n",
    "# doc_count is the number of documents to include the fine-tuning dataset\n",
    "# The higher the doc_count the larger the training dataset will be\n",
    "# The max document count is 693\n",
    "doc_count = 10\n",
    "\n",
    "def create_dataset(new_dataset_file, directory_path, docs_in_dataset):\n",
    "    doc_in_dataset = 0\n",
    "    \n",
    "    with open(new_dataset_file, 'w') as new_file:\n",
    "        file_list = os.listdir(directory_path)\n",
    "\n",
    "        for filename in file_list:\n",
    "            if doc_in_dataset < doc_count:\n",
    "                doc_in_dataset+=1\n",
    "                # Create the full file path by joining the directory path with the filename\n",
    "                file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "                # Check if the file is a regular file (not a directory)\n",
    "                if os.path.isfile(file_path):\n",
    "                    # Open the file in read mode\n",
    "                    with open(file_path, 'r') as file:\n",
    "                        text_content = file.read()\n",
    "\n",
    "                    # Write the content of each file to the new file\n",
    "                    new_file.write(text_content)\n",
    "                    new_file.write(\"\\n-----------------------------------------------------------------\\n\")\n",
    "\n",
    "# creats training dataset\n",
    "create_dataset(new_file_path, directory_path, doc_count)\n",
    "\n",
    "# creats validation dataset\n",
    "create_dataset(validation_file_path, validation_path, doc_count)\n",
    "\n",
    "print(\"Datasets created\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3267fe0-19a4-4b30-84a7-67818e3e3e8a",
   "metadata": {},
   "source": [
    "## Upload training data to S3\n",
    "In this section we upload the dataset that was created in the previous step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c44ba081-dc0a-46ac-96ad-9b9746913708",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# uploads training data to S3 so that model can be fine-tune using the dataset\n",
    "sagemaker_session.upload_data(train_file,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=training_folder)\n",
    "\n",
    "sagemaker_session.upload_data(validation_file_path,\n",
    "                              bucket=bucket_name, \n",
    "                              key_prefix=validation_folder)\n",
    "\n",
    "print(f\"Training Dataset: s3://{bucket_name}/{training_folder}/{s3_file}\")\n",
    "print(f\"Validation Dataset: s3://{bucket_name}/{validation_folder}/{s3_validation_file}\")\n",
    "print(\"Training data uploaded to S3\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2a66edbd-4d81-47e6-aa10-69dc18d7dd0b",
   "metadata": {},
   "source": [
    "## Setup Model to be tuned\n",
    "\n",
    "When selecting your instance type below ensure you have the minimal available to run based on your account quota. For some GPU based instances you may need to request an increase in the total number you can run in your account. This is true for spot instance type also which have a separate quota. \n",
    "\n",
    "You can request a service increase [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a0f44205-2436-4f4c-883d-9bb2eeacc67f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "s3://jumpstart-cache-prod-us-east-1/source-directory-tarballs/huggingface/transfer_learning/textgeneration1/prepack/v1.2.0/sourcedir.tar.gz\n",
      "s3://jumpstart-cache-prod-us-east-1/huggingface-training/train-huggingface-textgeneration1-gpt-j-6b.tar.gz\n"
     ]
    }
   ],
   "source": [
    "model_id, model_version = \"huggingface-textgeneration1-gpt-j-6b\", \"*\"\n",
    "\n",
    "from sagemaker import image_uris, model_uris, script_uris, hyperparameters\n",
    "\n",
    "# you can change the instance to a smaller instance - https://aws.amazon.com/ec2/instance-types/p3/\n",
    "# training_instance_type = \"ml.p3dn.24xlarge\" \n",
    "\n",
    "# G4 instance https://aws.amazon.com/ec2/instance-types/g4/\n",
    "# training_instance_type = \"ml.g4dn.12xlarge\" \n",
    "\n",
    "training_instance_type = \"ml.g4dn.16xlarge\"\n",
    "\n",
    "# training_instance_type = \"ml.g4dn.4xlarge\"\n",
    "\n",
    "# Retrieve the docker image for training\n",
    "train_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    image_scope=\"training\",\n",
    "    instance_type=training_instance_type,\n",
    ")\n",
    "\n",
    "# Retrieve the training script\n",
    "train_source_uri = script_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, script_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_source_uri)\n",
    "# Retrieve the pre-trained model tarball to further fine-tune\n",
    "train_model_uri = model_uris.retrieve(\n",
    "    model_id=model_id, model_version=model_version, model_scope=\"training\"\n",
    ")\n",
    "\n",
    "print(train_model_uri)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df73892e-1f0f-460f-9341-ab9670013d69",
   "metadata": {},
   "source": [
    "## Configure storage locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b509def4-9ad9-46f6-a296-1584783b0b5b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset location: s3://sagemaker-938247108506-us-east-1/training_dataset/\n",
      "Validation dataset location: s3://sagemaker-938247108506-us-east-1/validation_dataset/\n",
      "Model training output location: s3://sagemaker-938247108506-us-east-1/training_output/output\n"
     ]
    }
   ],
   "source": [
    "# bucket name \n",
    "bucket_name = f'sagemaker-{account_id}-{aws_region}'\n",
    "\n",
    "# location where training dataset will reside\n",
    "training_dataset_s3_path = f\"s3://{bucket_name}/training_dataset/\" \n",
    "\n",
    "# location where validation data will exist\n",
    "validation_dataset_s3_path = f\"s3://{bucket_name}/{validation_folder}/\"\n",
    "\n",
    "output_prefix = \"training_output\"\n",
    "\n",
    "s3_output_location = f\"s3://{bucket_name}/{output_prefix}/output\"\n",
    "\n",
    "print(f\"Training dataset location: {training_dataset_s3_path}\")\n",
    "print(f\"Validation dataset location: {validation_dataset_s3_path}\")\n",
    "print(f\"Model training output location: {s3_output_location}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d958b68c-0e43-4488-8f0d-0f1c3c8957ef",
   "metadata": {},
   "source": [
    "## Spot Training configuration\n",
    "If **use_spot_instances** is set to **True** below training will use spot instances.\n",
    "\n",
    "Note: If you are using spot instances for training you will need to store training checkpoints in case your spot instances are shutdown. This allows you to continue training where you left off if spot instances are terminated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "51034e0d-7b4d-4802-b330-3cf89e49296c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checkpoint storage location: s3://sagemaker-938247108506-us-east-1/training_output/checkpoints/huggingface-textgeneration1-gpt-j-6b-tr-2023-08-06-19-17-04-284\n"
     ]
    }
   ],
   "source": [
    "from sagemaker.utils import name_from_base\n",
    "training_job_name = name_from_base(f\"{model_id}-transfer-learning\")\n",
    "\n",
    "# set use_spot_instances to true if you are going to use spot instances for training\n",
    "# ensure you have the proper quota for the instance type you set for the training_instance_type variable\n",
    "# you can check your quota here https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas \n",
    "# by enter the instance type you plan on using\n",
    "use_spot_instances = True\n",
    "max_run = 360000 # in seconds\n",
    "max_wait = 720000 if use_spot_instances else None # in seconds\n",
    "\n",
    "checkpoint_s3_uri = None\n",
    "\n",
    "if use_spot_instances:\n",
    "    # sets the location where training checkpoint will be stored if using spot instances\n",
    "    checkpoint_s3_uri = f's3://{bucket_name}/{output_prefix}/checkpoints/{training_job_name}'\n",
    "    \n",
    "print (f'Checkpoint storage location: {checkpoint_s3_uri}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f090726-2433-455b-abf2-7ef00b2e7b69",
   "metadata": {},
   "source": [
    "## Train with Automatic Model Tuning (HPO)\n",
    "This section configures Automatic Model Tuning if you change from **use_auto_tuning = False** to **use_auto_tuning = True**. By default we set it to false for this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bcd4a283-f2e5-4bba-ab8e-321ae1111e60",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': '3', 'learning_rate': '6e-06', 'per_device_train_batch_size': '4', 'per_device_eval_batch_size': '8', 'warmup_ratio': '0.1', 'instruction_tuned': False, 'train_from_scratch': 'False', 'fp16': 'True', 'bf16': 'False', 'evaluation_strategy': 'steps', 'eval_steps': '20', 'gradient_accumulation_steps': '2', 'logging_steps': '10', 'weight_decay': '0.2', 'load_best_model_at_end': 'True', 'max_train_samples': '-1', 'max_val_samples': '-1', 'seed': '10', 'max_input_length': '-1', 'validation_split_ratio': '0.2', 'train_data_split_seed': '0', 'preprocessing_num_workers': 'None', 'max_steps': '-1', 'gradient_checkpointing': 'True', 'early_stopping_patience': '3', 'early_stopping_threshold': '0.0', 'adam_beta1': '0.9', 'adam_beta2': '0.999', 'adam_epsilon': '1e-08', 'max_grad_norm': '1.0', 'label_smoothing_factor': '0', 'logging_first_step': 'False', 'logging_nan_inf_filter': 'True', 'save_strategy': 'steps', 'save_steps': '500', 'save_total_limit': '1', 'dataloader_drop_last': 'False', 'dataloader_num_workers': '0', 'eval_accumulation_steps': 'None', 'auto_find_batch_size': 'False', 'lr_scheduler_type': 'constant_with_warmup', 'warmup_steps': '0'}\n"
     ]
    }
   ],
   "source": [
    "from sagemaker import hyperparameters\n",
    "\n",
    "# Set default hyper-parameters for fine-tuning the model\n",
    "hyperparameters = hyperparameters.retrieve_default(model_id=model_id, model_version=model_version)\n",
    "\n",
    "# Overriding default hyperparameters with custom values\n",
    "hyperparameters[\"epoch\"] = \"3\"\n",
    "hyperparameters[\"per_device_train_batch_size\"] = \"4\"\n",
    "\n",
    "# If you are training with domain specific datasets you will need parameter to set this to False\n",
    "hyperparameters[\"instruction_tuned\"] = False\n",
    "\n",
    "print(hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06e75710-2c0a-4ed1-8c84-2d05040aa62c",
   "metadata": {},
   "source": [
    "## Set hyperparameters\n",
    "This section configures any hyperparameter if you decide to use automated model tuning. In this example we are not but the code exist if you would like to test out automated model tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d2b12a8e-bc4b-4938-b4fc-d68dd51b3206",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.tuner import ContinuousParameter\n",
    "\n",
    "# Use AMT (automated model tuning) for tuning and selecting the best model\n",
    "use_auto_tuning = False\n",
    "\n",
    "# Define objective metric, based on which the best model will be selected.\n",
    "amt_metric_definitions = {\n",
    "    \"metrics\": [{\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"}],\n",
    "    \"type\": \"Minimize\",\n",
    "}\n",
    "\n",
    "# You can select from the hyperparameters supported by the model, and configure ranges of values to be searched for training the optimal model.(https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning-define-ranges.html)\n",
    "hyperparameter_ranges = {\n",
    "    \"learning_rate\": ContinuousParameter(0.00001, 0.0001, scaling_type=\"Logarithmic\")\n",
    "}\n",
    "\n",
    "# Increase the total number of training jobs run by AMT, for increased accuracy (and training time).\n",
    "max_jobs = 2\n",
    "# Change parallel training jobs run by AMT to reduce total training time, constrained by your account limits.\n",
    "# if max_jobs=max_parallel_jobs then Bayesian search turns to Random.\n",
    "max_parallel_jobs = 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87dedfd9-841e-4e8f-88a5-6cd17b548230",
   "metadata": {},
   "source": [
    "## Start Training\n",
    "Here we start our SageMaker training job to tune the model. Depending on how much data is being used, the size of your training instance and the number of instances used for training will dictate how long it will take to train/tune your new model.\n",
    "\n",
    "If your training job fails because you surpassed your qouta for that instance type you can request an increase in your quota for that instance type [here](https://us-east-1.console.aws.amazon.com/servicequotas/home/services/sagemaker/quotas). You can request an instance quota increase for regular training instances and spot instances.\n",
    "\n",
    "You may also run into an error stating lack of capacity for your instance type. If you receive this type of error you can re-run the cell until the training job starts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7e5a1fa-0461-4191-8121-b07c829d2dc8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "from sagemaker.tuner import HyperparameterTuner\n",
    "\n",
    "# the default value is File which downloads the entire dataset to the instance before training starts\n",
    "# setting input_mode to Pipe allows you to stream the training data to the instance during training\n",
    "training_file_input_mode = \"Pipe\"\n",
    "\n",
    "# defines model metrics that are used to evaluate the models performance\n",
    "metric_definitions = [\n",
    "    {\"Name\": \"train:loss\", \"Regex\": \"'loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:loss\", \"Regex\": \"'eval_loss': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:runtime\", \"Regex\": \"'eval_runtime': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:samples_per_second\", \"Regex\": \"'eval_samples_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "    {\"Name\": \"eval:eval_steps_per_second\", \"Regex\": \"'eval_steps_per_second': ([0-9]+\\.[0-9]+)\"},\n",
    "]\n",
    "\n",
    "# Create SageMaker Estimator instance\n",
    "tg_estimator = Estimator(\n",
    "    role=aws_role,\n",
    "    image_uri=train_image_uri,\n",
    "    source_dir=train_source_uri,\n",
    "    model_uri=train_model_uri,\n",
    "    entry_point=\"transfer_learning.py\",\n",
    "    instance_count=1,\n",
    "    volume_size=50,\n",
    "    instance_type=training_instance_type,\n",
    "    hyperparameters=hyperparameters,\n",
    "    output_path=s3_output_location,\n",
    "    base_job_name=training_job_name,\n",
    "    metric_definitions=metric_definitions,\n",
    "    checkpoint_s3_uri=checkpoint_s3_uri,\n",
    "    use_spot_instances=use_spot_instances,\n",
    "    max_run=max_run,\n",
    "    max_wait=max_wait,\n",
    "    input_mode=training_file_input_mode\n",
    ")\n",
    "\n",
    "# checks to see if you are using automated model tuning\n",
    "if use_auto_tuning:\n",
    "    hp_tuner = HyperparameterTuner(\n",
    "        tg_estimator,\n",
    "        amt_metric_definitions[\"metrics\"][0][\"Name\"],\n",
    "        hyperparameter_ranges,\n",
    "        amt_metric_definitions[\"metrics\"],\n",
    "        max_jobs=max_jobs,\n",
    "        max_parallel_jobs=max_parallel_jobs,\n",
    "        objective_type=amt_metric_definitions[\"type\"],\n",
    "        base_tuning_job_name=training_job_name\n",
    "    )\n",
    "    \n",
    "    print(\"Using hyerparameter tuning job\")\n",
    "    # Start a SageMaker Tuning job to search for the best hyperparameters\n",
    "    hp_tuner.fit({\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path }, logs=True)\n",
    "else:\n",
    "    # Start a SageMaker Training job by passing s3 path for the training dataset\n",
    "    tg_estimator.fit(\n",
    "        {\"train\": training_dataset_s3_path, \"validation\": validation_dataset_s3_path}, logs=True\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91bd7886-1895-4b93-932b-a41d9c97fdc8",
   "metadata": {},
   "source": [
    "## Review Training metrics\n",
    "Here we output the training metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1be261a3-68ef-4f9f-9683-363ad2e3677f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using estimator model\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>timestamp</th>\n",
       "      <th>metric_name</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>2.430900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>420.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>2.226200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>900.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.864300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1320.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.602000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1740.0</td>\n",
       "      <td>train:loss</td>\n",
       "      <td>1.312700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval:loss</td>\n",
       "      <td>2.035156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>900.0</td>\n",
       "      <td>eval:loss</td>\n",
       "      <td>2.130859</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval:runtime</td>\n",
       "      <td>9.962700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>900.0</td>\n",
       "      <td>eval:runtime</td>\n",
       "      <td>9.891600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.0</td>\n",
       "      <td>eval:samples_per_second</td>\n",
       "      <td>1.104000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   timestamp              metric_name     value\n",
       "0        0.0               train:loss  2.430900\n",
       "1      420.0               train:loss  2.226200\n",
       "2      900.0               train:loss  1.864300\n",
       "3     1320.0               train:loss  1.602000\n",
       "4     1740.0               train:loss  1.312700\n",
       "5        0.0                eval:loss  2.035156\n",
       "6      900.0                eval:loss  2.130859\n",
       "7        0.0             eval:runtime  9.962700\n",
       "8      900.0             eval:runtime  9.891600\n",
       "9        0.0  eval:samples_per_second  1.104000"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sagemaker import TrainingJobAnalytics\n",
    "\n",
    "if use_auto_tuning:\n",
    "    print(\"get the best trained model from the hyperparameter tuner\")\n",
    "    training_job_name = hp_tuner.best_training_job()\n",
    "else:\n",
    "    print(\"using estimator model\")\n",
    "    training_job_name = tg_estimator.latest_training_job.job_name\n",
    "\n",
    "df = TrainingJobAnalytics(training_job_name=training_job_name).dataframe()\n",
    "df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e56ac4f7-0250-406e-a55a-13ba4c71bb8c",
   "metadata": {},
   "source": [
    "## Deploy & run Inference on the fine-tuned model\n",
    "In this section we are deploying a model inference endpoint so that we can run inferences against the fine-tuned model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a9d7453f-681c-45c1-b171-f5739e775662",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker.image_uris:Ignoring unnecessary Python version: py39.\n",
      "INFO:sagemaker.image_uris:Ignoring unnecessary instance type: ml.g4dn.12xlarge.\n",
      "INFO:sagemaker:Creating model with name: sagemaker-jumpstart-2023-08-06-20-25-39-269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Endpoint name: fine-tuned-huggingface-textgeneration1--2023-08-06-20-25-39-269\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Creating endpoint-config with name fine-tuned-huggingface-textgeneration1--2023-08-06-20-25-39-269\n",
      "INFO:sagemaker:Creating endpoint with name fine-tuned-huggingface-textgeneration1--2023-08-06-20-25-39-269\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------!"
     ]
    }
   ],
   "source": [
    "inference_instance_type = \"ml.g4dn.12xlarge\"\n",
    "\n",
    "# Retrieve the docker container uri for inference\n",
    "deploy_image_uri = image_uris.retrieve(\n",
    "    region=None,\n",
    "    framework=None,\n",
    "    image_scope=\"inference\",\n",
    "    model_id=model_id,\n",
    "    model_version=model_version,\n",
    "    instance_type=inference_instance_type,\n",
    ")\n",
    "\n",
    "endpoint_name_after_finetune = name_from_base(f\"fine-tuned-{model_id}\")\n",
    "\n",
    "print(f\"Endpoint name: {endpoint_name_after_finetune}\" )\n",
    "\n",
    "# Deploy to SageMaker inference endpoint\n",
    "finetuned_predictor = (hp_tuner if use_auto_tuning else tg_estimator).deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type=inference_instance_type,\n",
    "    image_uri=deploy_image_uri,\n",
    "    endpoint_name=endpoint_name_after_finetune\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bc72ef4-217d-4f43-a7e0-61db33794f0f",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Inference Helper functions\n",
    "Creates two helper functions that will be used when we call the inference endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2fa6c064-11c3-41aa-a5d7-8522ace181dc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import boto3\n",
    "\n",
    "def query_endpoint_with_payload(encoded_json, endpoint_name):\n",
    "    client = boto3.client(\"runtime.sagemaker\")\n",
    "    \n",
    "    response = client.invoke_endpoint(\n",
    "        EndpointName=endpoint_name, ContentType=\"application/json\", Body=encoded_json\n",
    "    )\n",
    "    \n",
    "    return response\n",
    "\n",
    "\n",
    "def parse_response_texts(query_response):\n",
    "    generated_text = []\n",
    "    model_predictions = json.loads(query_response[\"Body\"].read())\n",
    "    return model_predictions[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b5fba258-0977-4a4f-804f-146124eb8c95",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tell me about the Matrimonial and Family Proceedings Act 1984: its effects, and yours. {#s1}\n",
      "====================================================================================\n",
      "\n",
      "The Matrimonial Causes Act 1973 created a system of parallel legal proceedings for civil and family cases, which many thought incompatible with the principle of a single trial.^[1](#FN1){ref-types]^A significant consequence was the extension of the time limits within which civil cases had to be concluded.By contrast, the Family Proceedings Act was intended to apply to the family court, and in particular to deal with family law matters such as the maintenance of children, and the disposal of the family home.It created a system of summary, and frequently accelerated, family court proceedings, which was designed to avoid what the law officers regarded as the unsatisfactory consequences of the parallel system.^[2](#FN2){ref-types]^It was a system which enabled parties to focus on the issues and the evidence.In time, it proved so effective that, at least in England and Wales, the system came to be seen as the only way to resolve family law cases expeditiously and without the negative features of the parallel system.As a result, the time limits imposed by the Matrimonial Causes Act for civil cases were extended to family proceedings.^[3](#FN3){ref-types]^There are, however, important differences between the two sets of time limits.Under the Matrimonial Causes Act, a party has 3 years from the date on which the cause of action arose to make an application for a writ of right, which may be followed by a full trial, unless the court allows the time to be extended for a period not exceeding one year.A judgment of absolute divorce can be obtained after six years.By contrast, a party to an accelerated family court case has five years from the date on which the proceedings were issued to make an application, and this period may be extended by a court order for up to a further five years.In other words, a party to an accelerated family court case can take legal action in respect of the matters which are the subject of the proceedings, and obtain a final judgment, up to 15 years after the date on which the proceedings were issued.These differences may have consequences for the person whose case has been accelerated.For example, the 15 year period means that the person may be an adult, and living independently, at the time\n",
      "\n",
      "------------------------------------------------------------------\n",
      "tell me about the Ellenborough Park [1956] case: [19] it was a claim for damages arising out of a collision between the plaintiff's motor car and a van. The claim was based on the allegation that the driver of the van was grossly intoxicated at the time of the collision. It was submitted on behalf of the plaintiff that the driver of the van was an agent of the defendants within the meaning of section 2(3)(b) of the Liquor Licensing Act 1997. The defendants objected on the grounds that the driver was not an agent in relation to the alleged breach of the statutory duty and also that the action was not one of which a court of competent jurisdiction could have taken cognisance. On the agency issue, Gray LJ (with whom the other members of the court agreed) held that the driver of the van was not an agent of the defendants within the meaning of section 2(3)(b) of the 1997 Act. He was not a servant in the technical sense, nor a partner, joint venturer or other associate, and he did not perform services for the defendants under a contract or agreement. However, he was an agent in the wider sense, namely, one who acted on behalf of the defendants, and that was enough to bring him within section 2(3)(b). The action was one of which a court of competent jurisdiction could have taken cognisance, and Gray LJ held that it was.I am in general agreement with Gray LJ's reasoning on this aspect of the case.But I do not think that the same can be said of the present case.It seems to me that the driver of the minicab was not a party to the contract between the company operating the minicabs and the minicab owner, nor was he in a position to suffer or give damages as a result of the alleged breach of the contract by the company.If the company had failed to provide a properly licensed minicab, it is hard to see what legal remedy the minicab owner would have against the driver.Although the minicab owner was said to be a co party to the contract with the minicab company, this in my view merely reflected the fact that the minicab owner was a shareholder in the minicab company and, as such, entitled to be a party to the contract.In my view, the driver was not an agent of the minicab company within the meaning of section\n",
      "\n",
      "------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "parameters = {\n",
    "    \"max_length\": 500,\n",
    "    \"num_return_sequences\": 1,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 0.8,\n",
    "    \"do_sample\": True,\n",
    "    \"temperature\": 1,\n",
    "}\n",
    "\n",
    "res_gpt_finetune = []\n",
    "    \n",
    "for quota_text in [\n",
    "    \"Tell me about the Matrimonial and Family Proceedings Act 1984\",\n",
    "    \"tell me about the Ellenborough Park [1956] case\",\n",
    "    # \"what was the case about that involved The Palmers Wood Oil Field\",\n",
    "    # \"what was the case about that involved Mohammed Jabar Ahmed, Mohammed Azmir Khan and Michael Marteen\",\n",
    "    # \"Tel me about Thirteenth Protocol to the European Convention on Human Rights (2004)\",\n",
    "]:\n",
    "    \n",
    "    payload = {\"text_inputs\": f\"{quota_text}:\", **parameters}\n",
    "    \n",
    "    query_response = query_endpoint_with_payload(\n",
    "        json.dumps(payload).encode(\"utf-8\"), endpoint_name_after_finetune\n",
    "    )\n",
    "    \n",
    "    generated_texts = parse_response_texts(query_response)[0][\"generated_text\"]\n",
    "    res_gpt_finetune.append(generated_texts)\n",
    "    \n",
    "    print(generated_texts)\n",
    "    print(\"\\n------------------------------------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10fed936-1acf-4d47-acc8-43ca10b0c8c9",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Clean-Up\n",
    "Here we are performing clean-up by deleting the fine-tuned model and deleting the inference endpoint that was deployed.\n",
    "\n",
    "Note: Leaving an inference endpoint running can be costly depending on the instance type you deployed your endpoint to. In this notebook we are using the ml.g5.12xlarge instance type for our inference endpoint. This is a GPU based instance which cost roughly $5.672 per hour to run at this time of publishing this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "44ec6142-d74f-4a40-8463-244e86c8c87c",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:sagemaker:Deleting model with name: sagemaker-jumpstart-2023-08-06-20-25-39-269\n",
      "INFO:sagemaker:Deleting endpoint configuration with name: fine-tuned-huggingface-textgeneration1--2023-08-06-20-25-39-269\n",
      "INFO:sagemaker:Deleting endpoint with name: fine-tuned-huggingface-textgeneration1--2023-08-06-20-25-39-269\n"
     ]
    }
   ],
   "source": [
    "# Delete the SageMaker endpoint and the attached resources\n",
    "finetuned_predictor.delete_model()\n",
    "finetuned_predictor.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3973f038-8152-4b86-8417-4ebd09085c4c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": "Python 3 (Data Science 3.0)",
   "language": "python",
   "name": "python3__SAGEMAKER_INTERNAL__arn:aws:sagemaker:us-east-1:081325390199:image/sagemaker-data-science-310-v1"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
